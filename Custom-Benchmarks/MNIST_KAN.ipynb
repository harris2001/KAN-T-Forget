{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99a1d7c2-e8bd-43c3-8849-24fadafcabde",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as T\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "os.chdir(\"../../pykan/kan/\")\n",
    "from KAN import KAN\n",
    "\n",
    "seed = 1234\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "batch_size_train = 32\n",
    "batch_size_test = 32\n",
    "lr = 1e-3\n",
    "momentum = 0.8\n",
    "\n",
    "tasks = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e533f2-fba9-4be9-a4e6-2484cbde2e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conventional model: NN(\n",
      "  (fc1): Linear(in_features=784, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "Device: cpu\n",
      "Device: cpu\n",
      "KAN model: KAN(\n",
      "  (biases): ModuleList(\n",
      "    (0): Linear(in_features=32, out_features=1, bias=False)\n",
      "    (1): Linear(in_features=10, out_features=1, bias=False)\n",
      "  )\n",
      "  (act_fun): ModuleList(\n",
      "    (0-1): 2 x KANLayer(\n",
      "      (base_fun): SiLU()\n",
      "    )\n",
      "  )\n",
      "  (base_fun): SiLU()\n",
      "  (symbolic_fun): ModuleList(\n",
      "    (0-1): 2 x Symbolic_KANLayer()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Create conventional MLP model\n",
    "class NN(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(NN, self).__init__()\n",
    "        # 28*28 input image, 64 output units, 10 output units\n",
    "        self.fc1 = nn.Linear(28*28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Flatten the image\n",
    "        x = x.view(-1, 28*28)\n",
    "        # Pass through the first layer\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # Pass through the second layer\n",
    "        x = self.fc2(x)\n",
    "        # Apply log softmax\n",
    "        x = F.log_softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = NN()\n",
    "model.to(device)\n",
    "print(f\"Conventional model: {model}\")\n",
    "\n",
    "# Create KAN model\n",
    "# KAN_model = FastKAN(layers_hidden=[28*28,64,10], grid_min=5, grid_max=7, device=device)\n",
    "# KAN_model.to(device)\n",
    "KAN_model = KAN(width=[28*28,32,10], grid=5, k=3, seed=0)\n",
    "\n",
    "print(f\"KAN model: {KAN_model}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b3ec13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Transform the MNIST dataset to a tensor and normalising it\n",
    "transform = T.Compose([\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.1307),(0.3081)), # Normalise samples using the mean of 0.1307 and std of 0.3081 - extracted after manual analysis\n",
    "    T.Lambda(lambda x: torch.flatten(x))\n",
    "])\n",
    "train_set = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
    "test_set = datasets.MNIST('../data', train=False, download=True, transform=transform)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma=momentum)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Pair numbers together and create 5 tasks\n",
    "tasks = [(i,i+1) for i in range(0,10,2)]\n",
    "\n",
    "# For training: fill splitted_train array with examples where the target value is in task i\n",
    "targets = train_set.targets.numpy()\n",
    "splitted_train = []\n",
    "for task in tasks:\n",
    "    indices = np.where(np.isin(targets, task))[0]\n",
    "    subset = [train_set[i] for i in indices]\n",
    "    splitted_train.append(subset)\n",
    "\n",
    "# For testing: fill splitted_train array with examples where the target value is in task i\n",
    "targets = test_set.targets.numpy()\n",
    "splitted_test = []\n",
    "for task in tasks:\n",
    "    indices = np.where(np.isin(targets, task))[0]\n",
    "    subset = [test_set[i] for i in indices]\n",
    "    splitted_test.append(subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c30fce7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([784])\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjYuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8o6BhiAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAbo0lEQVR4nO3df2yV9fn/8dfhRw+I7cFS2tMjPyyosICwDKU2YtXRUTrDRMiGjD9gcRJccVP8sXSbVOaybizZnK7DZZlUpoiSDIhkkmC1JW4FQoUxoquU1FFHWyYL50CxhbTv7x/9ej4eaIH7cE6vtjwfyTvh3Pd9nfvi7W1f3Oe+ex+fc84JAIBeNsi6AQDA1YkAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgIkh1g2cr7OzU8eOHVNqaqp8Pp91OwAAj5xzOnXqlEKhkAYN6vk8p88F0LFjxzR27FjrNgAAV6ixsVFjxozpcX2f+wguNTXVugUAQAJc6ud50gKovLxcN9xwg4YNG6bc3Fzt3bv3sur42A0ABoZL/TxPSgC9/vrrWrVqlUpLS/X+++9r+vTpKiws1PHjx5OxOwBAf+SSYObMma64uDj6uqOjw4VCIVdWVnbJ2nA47CQxGAwGo5+PcDh80Z/3CT8DOnv2rGpra1VQUBBdNmjQIBUUFKimpuaC7dvb2xWJRGIGAGDgS3gAffrpp+ro6FBWVlbM8qysLDU3N1+wfVlZmQKBQHRwBxwAXB3M74IrKSlROByOjsbGRuuWAAC9IOG/B5SRkaHBgwerpaUlZnlLS4uCweAF2/v9fvn9/kS3AQDo4xJ+BpSSkqIZM2aosrIyuqyzs1OVlZXKy8tL9O4AAP1UUp6EsGrVKi1dulS33nqrZs6cqeeee06tra36zne+k4zdAQD6oaQE0KJFi/Tf//5Xq1evVnNzs7785S9rx44dF9yYAAC4evmcc866iS+KRCIKBALWbQAArlA4HFZaWlqP683vggMAXJ0IIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGBiiHUDAC5Pamqq55qVK1fGta+f//zncdV5dejQIc81c+bM8VzT1NTkuQbJxxkQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEz7nnLNu4osikYgCgYB1G8Blu/766z3XbN682XPNpEmTPNdcd911nmv6unA47LnmW9/6Vlz72rlzZ1x16BIOh5WWltbjes6AAAAmCCAAgImEB9Azzzwjn88XMyZPnpzo3QAA+rmkfCHdlClT9Pbbb//fTobwvXcAgFhJSYYhQ4YoGAwm460BAANEUq4BHT58WKFQSBMmTNCSJUt09OjRHrdtb29XJBKJGQCAgS/hAZSbm6uKigrt2LFD69atU0NDg+68806dOnWq2+3LysoUCASiY+zYsYluCQDQByU8gIqKivTNb35T06ZNU2Fhof7617/q5MmTeuONN7rdvqSkROFwODoaGxsT3RIAoA9K+t0BI0eO1M0336z6+vpu1/v9fvn9/mS3AQDoY5L+e0CnT5/WkSNHlJ2dnexdAQD6kYQH0BNPPKHq6mp9/PHH+vvf/677779fgwcP1uLFixO9KwBAP5bwj+A++eQTLV68WCdOnNDo0aM1a9Ys7d69W6NHj070rgAA/VjCA2jTpk2JfkugT1u/fr3nmttvv91zTVtbm+eabdu2ea6Rum4O8mrDhg2ea2699VbPNfE8rDglJcVzDZKPZ8EBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwkfQvpAP6k6997Wuea/Ly8pLQyYWeffZZzzVlZWVJ6KR7L730kueaeB5G+uabb3quqamp8VyD5OMMCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABggqdh92FpaWmea9ra2jzXnD171nPNQJWRkeG5ZsSIEUno5EIff/xxr+ynN7W2tnquWb16teea//3vf55rkHycAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBw0j7sG984xuea/bu3eu55qOPPvJcA5xv165dnmsWLFjguaajo8NzTWlpqecaSVqzZk1cdbg8nAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwcNI+7BXXnnFugXgsn3wwQeeawoLCz3XvPTSS55rIpGI5xqJh5EmG2dAAAATBBAAwITnANq1a5fmzZunUCgkn8+nrVu3xqx3zmn16tXKzs7W8OHDVVBQoMOHDyeqXwDAAOE5gFpbWzV9+nSVl5d3u37t2rV6/vnn9eKLL2rPnj0aMWKECgsL1dbWdsXNAgAGDs83IRQVFamoqKjbdc45Pffcc/rJT36i++67T5K0YcMGZWVlaevWrXrggQeurFsAwICR0GtADQ0Nam5uVkFBQXRZIBBQbm6uampquq1pb29XJBKJGQCAgS+hAdTc3CxJysrKilmelZUVXXe+srIyBQKB6Bg7dmwiWwIA9FHmd8GVlJQoHA5HR2Njo3VLAIBekNAACgaDkqSWlpaY5S0tLdF15/P7/UpLS4sZAICBL6EBlJOTo2AwqMrKyuiySCSiPXv2KC8vL5G7AgD0c57vgjt9+rTq6+ujrxsaGnTgwAGlp6dr3LhxevTRR/Wzn/1MN910k3JycvT0008rFApp/vz5iewbANDPeQ6gffv26Z577om+XrVqlSRp6dKlqqio0FNPPaXW1lYtX75cJ0+e1KxZs7Rjxw4NGzYscV0DAPo9n3POWTfxRZFIRIFAwLoNXKVmzJjhueatt97yXJORkeG5Zt26dZ5riouLPdfEa8OGDZ5rlixZ4rnG5/N5rvnwww8910jSlClT4qpDl3A4fNHr+uZ3wQEArk4EEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABOev44BGMhqa2s91/znP//xXBPP07CXL1/uueaf//yn5xpJ+vGPf+y5JhQKea6J58nW8Xj//fd7ZT/whjMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJnzOOWfdxBdFIhEFAgHrNoDLtnTpUs8169evT0InF2pqaoqrLjs7O8GdJM6mTZs816xYsSKufUUikbjq0CUcDistLa3H9ZwBAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMDHEugGgvzt27Jh1Cz3qzYeKfvrpp55r3nrrLc813//+9z3X8FDRvokzIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACZ4GCnwBUVFRZ5rnnzyySR0khjOubjq4nnA6uLFiz3XvPfee55rMHBwBgQAMEEAAQBMeA6gXbt2ad68eQqFQvL5fNq6dWvM+mXLlsnn88WMuXPnJqpfAMAA4TmAWltbNX36dJWXl/e4zdy5c9XU1BQdr7322hU1CQAYeDzfhFBUVHTJC7V+v1/BYDDupgAAA19SrgFVVVUpMzNTkyZN0sMPP6wTJ070uG17e7sikUjMAAAMfAkPoLlz52rDhg2qrKzUL3/5S1VXV6uoqEgdHR3dbl9WVqZAIBAdY8eOTXRLAIA+KOG/B/TAAw9E/3zLLbdo2rRpmjhxoqqqqjR79uwLti8pKdGqVauiryORCCEEAFeBpN+GPWHCBGVkZKi+vr7b9X6/X2lpaTEDADDwJT2APvnkE504cULZ2dnJ3hUAoB/x/BHc6dOnY85mGhoadODAAaWnpys9PV1r1qzRwoULFQwGdeTIET311FO68cYbVVhYmNDGAQD9m+cA2rdvn+65557o68+v3yxdulTr1q3TwYMH9fLLL+vkyZMKhUKaM2eOnn32Wfn9/sR1DQDo93wu3qcVJkkkElEgELBuA33IkCHe75W566674trX+U/2uBwjRoyIa1+9ob29Pa664cOHJ7gTXI3C4fBFr+vzLDgAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgImEfyU3cDH33nuv55onnnjCc028T8MG0Hs4AwIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCh5EibosXL/ZcU1FR4blm6NChnmvi9ec//9lzTXp6uueaeB7KCgw0nAEBAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwwcNIoU2bNsVVF88DNXvrwaKPP/54XHUvvPCC55p77rnHcw0PIwU4AwIAGCGAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCh5EOMLNmzfJcU1RUFNe+RowY4bnm0KFDnmu++93veq7Zt2+f5xpJysjI8FxTUlIS1756w+9+9zvrFoAecQYEADBBAAEATHgKoLKyMt12221KTU1VZmam5s+fr7q6upht2traVFxcrFGjRunaa6/VwoUL1dLSktCmAQD9n6cAqq6uVnFxsXbv3q2dO3fq3LlzmjNnjlpbW6PbPPbYY3rzzTe1efNmVVdX69ixY1qwYEHCGwcA9G+ebkLYsWNHzOuKigplZmaqtrZW+fn5CofD+tOf/qSNGzfqq1/9qiRp/fr1+tKXvqTdu3fr9ttvT1znAIB+7YquAYXDYUlSenq6JKm2tlbnzp1TQUFBdJvJkydr3Lhxqqmp6fY92tvbFYlEYgYAYOCLO4A6Ozv16KOP6o477tDUqVMlSc3NzUpJSdHIkSNjts3KylJzc3O371NWVqZAIBAdY8eOjbclAEA/EncAFRcX69ChQ9q0adMVNVBSUqJwOBwdjY2NV/R+AID+Ia5fRF25cqW2b9+uXbt2acyYMdHlwWBQZ8+e1cmTJ2POglpaWhQMBrt9L7/fL7/fH08bAIB+zNMZkHNOK1eu1JYtW/TOO+8oJycnZv2MGTM0dOhQVVZWRpfV1dXp6NGjysvLS0zHAIABwdMZUHFxsTZu3Kht27YpNTU1el0nEAho+PDhCgQCevDBB7Vq1Sqlp6crLS1NjzzyiPLy8rgDDgAQw1MArVu3TpJ09913xyxfv369li1bJkn6zW9+o0GDBmnhwoVqb29XYWGhfv/73yekWQDAwOEpgJxzl9xm2LBhKi8vV3l5edxNIX7n34F4OVJTUxPfSALl5uZ6rjl9+nRc+4rnH0v5+flx7curEydOeK7h/0P0ZTwLDgBgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgIq5vREXf9Y9//MNzzUcffRTXvm6++WbPNVOnTvVc89vf/tZzzUD08ssve675+OOPE98IkCCcAQEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADDBw0gHmMbGRs81a9asiWtfJSUlnmvieRhpX3f48GHPNX/84x8912zfvt1zDdCXcQYEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADAhM8556yb+KJIJKJAIGDdBgDgCoXDYaWlpfW4njMgAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGCCAAIAmCCAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCY8BRAZWVluu2225SamqrMzEzNnz9fdXV1Mdvcfffd8vl8MWPFihUJbRoA0P95CqDq6moVFxdr9+7d2rlzp86dO6c5c+aotbU1ZruHHnpITU1N0bF27dqENg0A6P+GeNl4x44dMa8rKiqUmZmp2tpa5efnR5dfc801CgaDiekQADAgXdE1oHA4LElKT0+PWf7qq68qIyNDU6dOVUlJic6cOdPje7S3tysSicQMAMBVwMWpo6PD3Xvvve6OO+6IWf6HP/zB7dixwx08eNC98sor7vrrr3f3339/j+9TWlrqJDEYDAZjgI1wOHzRHIk7gFasWOHGjx/vGhsbL7pdZWWlk+Tq6+u7Xd/W1ubC4XB0NDY2mk8ag8FgMK58XCqAPF0D+tzKlSu1fft27dq1S2PGjLnotrm5uZKk+vp6TZw48YL1fr9ffr8/njYAAP2YpwByzumRRx7Rli1bVFVVpZycnEvWHDhwQJKUnZ0dV4MAgIHJUwAVFxdr48aN2rZtm1JTU9Xc3CxJCgQCGj58uI4cOaKNGzfq61//ukaNGqWDBw/qscceU35+vqZNm5aUvwAAoJ/yct1HPXzOt379euecc0ePHnX5+fkuPT3d+f1+d+ONN7onn3zykp8DflE4HDb/3JLBYDAYVz4u9bPf9/+Dpc+IRCIKBALWbQAArlA4HFZaWlqP63kWHADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADABAEEADBBAAEATBBAAAATBBAAwAQBBAAwQQABAEwQQAAAEwQQAMAEAQQAMEEAAQBMEEAAABMEEADARJ8LIOecdQsAgAS41M/zPhdAp06dsm4BAJAAl/p57nN97JSjs7NTx44dU2pqqnw+X8y6SCSisWPHqrGxUWlpaUYd2mMeujAPXZiHLsxDl74wD845nTp1SqFQSIMG9XyeM6QXe7osgwYN0pgxYy66TVpa2lV9gH2OeejCPHRhHrowD12s5yEQCFxymz73ERwA4OpAAAEATPSrAPL7/SotLZXf77duxRTz0IV56MI8dGEeuvSneehzNyEAAK4O/eoMCAAwcBBAAAATBBAAwAQBBAAw0W8CqLy8XDfccIOGDRum3Nxc7d2717qlXvfMM8/I5/PFjMmTJ1u3lXS7du3SvHnzFAqF5PP5tHXr1pj1zjmtXr1a2dnZGj58uAoKCnT48GGbZpPoUvOwbNmyC46PuXPn2jSbJGVlZbrtttuUmpqqzMxMzZ8/X3V1dTHbtLW1qbi4WKNGjdK1116rhQsXqqWlxajj5Licebj77rsvOB5WrFhh1HH3+kUAvf7661q1apVKS0v1/vvva/r06SosLNTx48etW+t1U6ZMUVNTU3S899571i0lXWtrq6ZPn67y8vJu169du1bPP/+8XnzxRe3Zs0cjRoxQYWGh2traernT5LrUPEjS3LlzY46P1157rRc7TL7q6moVFxdr9+7d2rlzp86dO6c5c+aotbU1us1jjz2mN998U5s3b1Z1dbWOHTumBQsWGHadeJczD5L00EMPxRwPa9euNeq4B64fmDlzpisuLo6+7ujocKFQyJWVlRl21ftKS0vd9OnTrdswJclt2bIl+rqzs9MFg0H3q1/9Krrs5MmTzu/3u9dee82gw95x/jw459zSpUvdfffdZ9KPlePHjztJrrq62jnX9d9+6NChbvPmzdFtPvzwQyfJ1dTUWLWZdOfPg3PO3XXXXe4HP/iBXVOXoc+fAZ09e1a1tbUqKCiILhs0aJAKCgpUU1Nj2JmNw4cPKxQKacKECVqyZImOHj1q3ZKphoYGNTc3xxwfgUBAubm5V+XxUVVVpczMTE2aNEkPP/ywTpw4Yd1SUoXDYUlSenq6JKm2tlbnzp2LOR4mT56scePGDejj4fx5+Nyrr76qjIwMTZ06VSUlJTpz5oxFez3qcw8jPd+nn36qjo4OZWVlxSzPysrSv/71L6OubOTm5qqiokKTJk1SU1OT1qxZozvvvFOHDh1SamqqdXsmmpubJanb4+PzdVeLuXPnasGCBcrJydGRI0f0ox/9SEVFRaqpqdHgwYOt20u4zs5OPfroo7rjjjs0depUSV3HQ0pKikaOHBmz7UA+HrqbB0n69re/rfHjxysUCungwYP64Q9/qLq6Ov3lL38x7DZWnw8g/J+ioqLon6dNm6bc3FyNHz9eb7zxhh588EHDztAXPPDAA9E/33LLLZo2bZomTpyoqqoqzZ4927Cz5CguLtahQ4euiuugF9PTPCxfvjz651tuuUXZ2dmaPXu2jhw5ookTJ/Z2m93q8x/BZWRkaPDgwRfcxdLS0qJgMGjUVd8wcuRI3Xzzzaqvr7duxcznxwDHx4UmTJigjIyMAXl8rFy5Utu3b9e7774b8/UtwWBQZ8+e1cmTJ2O2H6jHQ0/z0J3c3FxJ6lPHQ58PoJSUFM2YMUOVlZXRZZ2dnaqsrFReXp5hZ/ZOnz6tI0eOKDs727oVMzk5OQoGgzHHRyQS0Z49e6764+OTTz7RiRMnBtTx4ZzTypUrtWXLFr3zzjvKycmJWT9jxgwNHTo05nioq6vT0aNHB9TxcKl56M6BAwckqW8dD9Z3QVyOTZs2Ob/f7yoqKtwHH3zgli9f7kaOHOmam5utW+tVjz/+uKuqqnINDQ3ub3/7mysoKHAZGRnu+PHj1q0l1alTp9z+/fvd/v37nST361//2u3fv9/9+9//ds4594tf/MKNHDnSbdu2zR08eNDdd999Licnx3322WfGnSfWxebh1KlT7oknnnA1NTWuoaHBvf322+4rX/mKu+mmm1xbW5t16wnz8MMPu0Ag4KqqqlxTU1N0nDlzJrrNihUr3Lhx49w777zj9u3b5/Ly8lxeXp5h14l3qXmor693P/3pT92+fftcQ0OD27Ztm5swYYLLz8837jxWvwgg55x74YUX3Lhx41xKSoqbOXOm2717t3VLvW7RokUuOzvbpaSkuOuvv94tWrTI1dfXW7eVdO+++66TdMFYunSpc67rVuynn37aZWVlOb/f72bPnu3q6upsm06Ci83DmTNn3Jw5c9zo0aPd0KFD3fjx491DDz004P6R1t3fX5Jbv359dJvPPvvMfe9733PXXXedu+aaa9z999/vmpqa7JpOgkvNw9GjR11+fr5LT093fr/f3Xjjje7JJ5904XDYtvHz8HUMAAATff4aEABgYCKAAAAmCCAAgAkCCABgggACAJgggAAAJgggAIAJAggAYIIAAgCYIIAAACYIIACACQIIAGDi/wFhuKrsrhhawgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualise a random image to check if the data is loaded correctly\n",
    "rand_image = random.choice(splitted_train[4])\n",
    "print(rand_image[0].shape)\n",
    "plt.imshow(rand_image[0].numpy().reshape(28,28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e69be73-93da-4577-99f1-53ba1a5976f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on 0\n",
      "12665 2115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "description:   0%|                                                           | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "# Train the KAN\n",
    "performance = []\n",
    "for task in range(len(tasks)):\n",
    "    print(f\"Training on {task}\")\n",
    "\n",
    "    # Create dataloader with the splitted training and testing sets\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        splitted_train[task],\n",
    "        batch_size=batch_size_train,\n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # Accumulate testing samples into performance array to evaluate forgetting of previous tasks\n",
    "    performance.extend(splitted_test[task])\n",
    "    \n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        performance, \n",
    "        batch_size=batch_size_test, \n",
    "        shuffle=True\n",
    "    )\n",
    "\n",
    "    # # Visualise a random image to check if the data is loaded correctly\n",
    "    # rand_image = random.choice(performance)\n",
    "    # print(rand_image[0][0].shape)\n",
    "    # plt.imshow(rand_image[0][0].numpy().reshape(28,28))\n",
    "    # plt.show()\n",
    "    \n",
    "    print(len(train_loader.dataset), len(test_loader.dataset))\n",
    "    \n",
    "    for epoch in range(10):\n",
    "        # Train the conventional model\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc_conv = correct / len(train_loader.dataset)\n",
    "        \n",
    "\n",
    "\n",
    "        # Train the KAN model\n",
    "        dataset = {'train_input': torch.tensor([]), 'train_label': torch.tensor([]), 'test_input': torch.tensor([]), 'test_label': torch.tensor([])}\n",
    "        train_input = []\n",
    "        train_label = []\n",
    "        test_input = []\n",
    "        test_label = []\n",
    "        for data, target in train_loader:\n",
    "            train_input.append(data)\n",
    "            train_label.append(target)\n",
    "        dataset['train_input'] = torch.cat(train_input)\n",
    "        dataset['train_label'] = torch.cat(train_label)\n",
    "        for data, target in test_loader:\n",
    "            test_input.append(data)\n",
    "            test_label.append(target)\n",
    "        dataset['test_input'] = torch.cat(test_input)\n",
    "        dataset['test_label'] = torch.cat(test_label)\n",
    "\n",
    "        KAN_model.train(dataset, opt=\"Adam\", steps=10, update_grid=True)\n",
    "        train_loss = 0\n",
    "        correct = 0\n",
    "\n",
    "        for data, target in train_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = KAN_model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item() * data.size(0)\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        train_acc_kan = correct / len(train_loader.dataset)\n",
    "\n",
    "        train_acc = [train_acc_conv, train_acc_kan]\n",
    "\n",
    "        KAN_model.eval()\n",
    "        model.eval()\n",
    "        test_loss = [0,0]\n",
    "        correct = [0,0]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                data, target = data.to(device), target.to(device)\n",
    "                output = [model(data),KAN_model(data)]\n",
    "                test_loss[0] += loss_fn(output[0], target).item() * data.size(0)\n",
    "                test_loss[1] += loss_fn(output[1], target).item() * data.size(0)\n",
    "                pred = [output[0].argmax(dim=1, keepdim=True), output[1].argmax(dim=1, keepdim=True)]\n",
    "                correct[0] += pred[0].eq(target.view_as(pred[0])).sum().item()\n",
    "                correct[1] += pred[1].eq(target.view_as(pred[1])).sum().item()\n",
    "        test_loss[0] /= len(test_loader.dataset)\n",
    "        test_loss[1] /= len(test_loader.dataset)\n",
    "        \n",
    "        test_acc = [0,0]\n",
    "        test_acc[0] = correct[0] / len(test_loader.dataset)\n",
    "        test_acc[1] = correct[1] / len(test_loader.dataset)\n",
    "\n",
    "        # Update the learning rate\n",
    "        scheduler.step()\n",
    "        # print training and testing accuracy for each model\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        print(f\"Conventional model: Train accuracy: {train_acc[0]*100:.2f}, Test accuracy: {test_acc[0]*100:.2f}    VS   KAN model: Train accuracy: {train_acc[1]*100:.2f}, Test accuracy: {test_acc[1]*100:.2f}\")\n",
    "        # Plot the accuracy of the KAN model after each epoch\n",
    "        # plt.plot(epoch, train_acc, 'b-')\n",
    "        # plt.plot(epoch, test_acc, 'r-')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Testing Accuracy (KAN Model)')\n",
    "    plt.legend(['Train', 'Test'])\n",
    "    # plt.show()\n",
    "    \n",
    "    \n",
    "torch.save(model.state_dict(), \"mnist_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c430a65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FastKAN(\n",
       "  (layers): ModuleList(\n",
       "    (0): FastKANLayer(\n",
       "      (layernorm): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
       "      (rbf): RadialBasisFunction()\n",
       "      (spline_linear): SplineLinear(in_features=6272, out_features=64, bias=False)\n",
       "      (base_linear): Linear(in_features=784, out_features=64, bias=True)\n",
       "    )\n",
       "    (1): FastKANLayer(\n",
       "      (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
       "      (rbf): RadialBasisFunction()\n",
       "      (spline_linear): SplineLinear(in_features=512, out_features=10, bias=False)\n",
       "      (base_linear): Linear(in_features=64, out_features=10, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = FastKAN(layers_hidden=[28*28,64,10], grid_min=3, grid_max=5, device=device)\n",
    "model.load_state_dict(torch.load(\"mnist_model.pth\"))\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40e28078",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Layer  FastKAN(\n",
      "  (layers): ModuleList(\n",
      "    (0): FastKANLayer(\n",
      "      (layernorm): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
      "      (rbf): RadialBasisFunction()\n",
      "      (spline_linear): SplineLinear(in_features=6272, out_features=64, bias=False)\n",
      "      (base_linear): Linear(in_features=784, out_features=64, bias=True)\n",
      "    )\n",
      "    (1): FastKANLayer(\n",
      "      (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "      (rbf): RadialBasisFunction()\n",
      "      (spline_linear): SplineLinear(in_features=512, out_features=10, bias=False)\n",
      "      (base_linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "====> Layer  ModuleList(\n",
      "  (0): FastKANLayer(\n",
      "    (layernorm): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
      "    (rbf): RadialBasisFunction()\n",
      "    (spline_linear): SplineLinear(in_features=6272, out_features=64, bias=False)\n",
      "    (base_linear): Linear(in_features=784, out_features=64, bias=True)\n",
      "  )\n",
      "  (1): FastKANLayer(\n",
      "    (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "    (rbf): RadialBasisFunction()\n",
      "    (spline_linear): SplineLinear(in_features=512, out_features=10, bias=False)\n",
      "    (base_linear): Linear(in_features=64, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "====> Layer  FastKANLayer(\n",
      "  (layernorm): LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
      "  (rbf): RadialBasisFunction()\n",
      "  (spline_linear): SplineLinear(in_features=6272, out_features=64, bias=False)\n",
      "  (base_linear): Linear(in_features=784, out_features=64, bias=True)\n",
      ")\n",
      "====> Layer  LayerNorm((784,), eps=1e-05, elementwise_affine=True)\n",
      "====> Layer  RadialBasisFunction()\n",
      "====> Layer  SplineLinear(in_features=6272, out_features=64, bias=False)\n",
      "====> Layer  Linear(in_features=784, out_features=64, bias=True)\n",
      "====> Layer  FastKANLayer(\n",
      "  (layernorm): LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "  (rbf): RadialBasisFunction()\n",
      "  (spline_linear): SplineLinear(in_features=512, out_features=10, bias=False)\n",
      "  (base_linear): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "====> Layer  LayerNorm((64,), eps=1e-05, elementwise_affine=True)\n",
      "====> Layer  RadialBasisFunction()\n",
      "====> Layer  SplineLinear(in_features=512, out_features=10, bias=False)\n",
      "====> Layer  Linear(in_features=64, out_features=10, bias=True)\n"
     ]
    }
   ],
   "source": [
    "layers = []\n",
    "for layer in model.modules():\n",
    "    layers.append(layer)\n",
    "    print(\"====> Layer \", layer)\n",
    "layer = layers[1]\n",
    "# # for i in range(layer.input_dim):\n",
    "# for j in range(layer.output_dim):\n",
    "#     # layer.set_curve(i, j, torch.tensor([0.0, 1.0, 0.0], device=device))\n",
    "#     x, y = layer.plot_curve(0, j, num_pts=100, num_extrapolate_bins=3)\n",
    "#     plt.plot(x.to('cpu').numpy(), y.to('cpu').numpy(), label=f\"Last Layer, Input 0, Output {j}\")\n",
    "# # plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431db2d1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FastKANLayer' object has no attribute 'curve'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m output \u001b[38;5;241m=\u001b[39m model(black_image)\n\u001b[1;32m      5\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurve\u001b[49m(output, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m      7\u001b[0m output \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m      8\u001b[0m output \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1709\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1707\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1708\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1709\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FastKANLayer' object has no attribute 'curve'"
     ]
    }
   ],
   "source": [
    "# for each pixel of the given input image, given a layer an input and an output, give me the gradient of the weights\n",
    "def get_gradient(model, layer, input, output):\n",
    "    model.zero_grad()\n",
    "    output = model(input)\n",
    "    output = F.softmax(output, dim=1)\n",
    "    loss = F.cross_entropy(output, output)\n",
    "    loss.backward()\n",
    "    return layer.weight.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ee585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
