{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thealch3mist/University/Thesis/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "train - len 50\n",
      "test - len 50\n",
      "{'train': <avalanche.benchmarks.scenarios.deprecated.new_classes.nc_scenario.NCStream object at 0x7c68561c10a0>, 'test': <avalanche.benchmarks.scenarios.deprecated.new_classes.nc_scenario.NCStream object at 0x7c68561c1070>}\n",
      "Starting experiment...\n",
      "Experience <avalanche.benchmarks.scenarios.deprecated.new_classes.nc_scenario.NCExperience object at 0x7c68561c2e10>\n",
      "-- >> Start of training phase << --\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[100, 3, 32, 32]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 133\u001b[0m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m experience \u001b[38;5;129;01min\u001b[39;00m benchmark\u001b[38;5;241m.\u001b[39mtrain_stream:\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# train returns a dictionary which contains all the metric values\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExperience \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexperience\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 133\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mcl_strategy_KAN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperience\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    134\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    136\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mComputing accuracy on the whole test set\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:211\u001b[0m, in \u001b[0;36mBaseSGDTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\n\u001b[1;32m    204\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    205\u001b[0m     experiences: Union[TDatasetExperience, Iterable[TDatasetExperience]],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    210\u001b[0m ):\n\u001b[0;32m--> 211\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    212\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluator\u001b[38;5;241m.\u001b[39mget_last_metrics()\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/base.py:163\u001b[0m, in \u001b[0;36mBaseTemplate.train\u001b[0;34m(self, experiences, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperience \u001b[38;5;129;01min\u001b[39;00m experiences_list:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 163\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_exp\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_streams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_exp(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    165\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/base_sgd.py:337\u001b[0m, in \u001b[0;36mBaseSGDTemplate._train_exp\u001b[0;34m(self, experience, eval_streams, **kwargs)\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_stop_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m--> 337\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_training_epoch(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/update_type/sgd_update.py:31\u001b[0m, in \u001b[0;36mSGDUpdate.training_epoch\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Forward\u001b[39;00m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_before_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmb_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_after_forward(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# Loss & Backward\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/problem_type/supervised_problem.py:47\u001b[0m, in \u001b[0;36mSupervisedProblem.forward\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     46\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the model's output given the current mini-batch.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mavalanche_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmb_task_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/avalanche/models/utils.py:22\u001b[0m, in \u001b[0;36mavalanche_forward\u001b[0;34m(model, x, task_labels)\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model(x, task_labels)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# no task labels\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Thesis/KANT_Forget/Custom-Benchmarks/Scaled_KAN.py:163\u001b[0m, in \u001b[0;36mFastKAN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers:\n\u001b[0;32m--> 163\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Thesis/KANT_Forget/Custom-Benchmarks/Scaled_KAN.py:94\u001b[0m, in \u001b[0;36mFastKANLayer.forward\u001b[0;34m(self, x, use_layernorm)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, use_layernorm\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[1;32m     93\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m use_layernorm:\n\u001b[0;32m---> 94\u001b[0m         spline_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbf(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayernorm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     95\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     96\u001b[0m         spline_basis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrbf(x)\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/modules/normalization.py:201\u001b[0m, in \u001b[0;36mLayerNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/University/Thesis/venv/lib/python3.12/site-packages/torch/nn/functional.py:2573\u001b[0m, in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_variadic(\u001b[38;5;28minput\u001b[39m, weight, bias):\n\u001b[1;32m   2570\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m   2571\u001b[0m         layer_norm, (\u001b[38;5;28minput\u001b[39m, weight, bias), \u001b[38;5;28minput\u001b[39m, normalized_shape, weight\u001b[38;5;241m=\u001b[39mweight, bias\u001b[38;5;241m=\u001b[39mbias, eps\u001b[38;5;241m=\u001b[39meps\n\u001b[1;32m   2572\u001b[0m     )\n\u001b[0;32m-> 2573\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayer_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalized_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given normalized_shape=[1024], expected input with shape [*, 1024], but got input of size[100, 3, 32, 32]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from avalanche.models import IncrementalClassifier, MultiHeadClassifier, MultiTaskModule\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "from Scaled_KAN import *\n",
    "os.chdir(\"Split_MNIST\")\n",
    "from avalanche.benchmarks.datasets import MNIST\n",
    "from avalanche.benchmarks.datasets.torchvision_wrapper import CIFAR100\n",
    "\n",
    "from avalanche.benchmarks.datasets.dataset_utils import default_dataset_location\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.benchmarks.utils import as_classification_dataset, AvalancheDataset\n",
    "\n",
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC  # and many more!\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, EWCPlugin\n",
    "\n",
    "from avalanche.logging import InteractiveLogger\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from avalanche.benchmarks.classic import SplitMNIST, SplitCIFAR100\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, \\\n",
    "accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, disk_usage_metrics\n",
    "\n",
    "# from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_with_validation_stream\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "################################\n",
    "# DEFINE THE MODELS TO BE USED #\n",
    "################################\n",
    "\n",
    "IN_DIM = 32*32\n",
    "model_KAN = FastKAN(layers_hidden=[IN_DIM,32,10], num_grids=4, device=device)\n",
    "model_incremental = IncrementalClassifier(in_features=IN_DIM)\n",
    "model_multihead = MultiHeadClassifier(in_features=IN_DIM)\n",
    "# model_multi_task = MultiTaskModule(in_features=IN_DIM)\n",
    "\n",
    "################################\n",
    "# DOWNLOAD DATASETS TO BE USED #\n",
    "################################\n",
    "\n",
    "# datadir = default_dataset_location('mnist')\n",
    "# train_MNIST = as_classification_dataset(MNIST(datadir, train=True, download=True))\n",
    "# test_MNIST = as_classification_dataset(MNIST(datadir, train=False, download=True))\n",
    "\n",
    "# datadir = default_dataset_location('cifar100')\n",
    "# train_CIFAR100 = as_classification_dataset(CIFAR100(datadir, train=True, download=True))\n",
    "# test_CIFAR100 = as_classification_dataset(CIFAR100(datadir, train=False, download=True))\n",
    "\n",
    "\n",
    "# ################################\n",
    "#     CREATE THE BENCHMARK     #\n",
    "################################\n",
    "\n",
    "# benchmark = benchmark_from_datasets(\n",
    "#     train=[train_MNIST, train_CIFAR100],\n",
    "#     test=[test_MNIST, test_CIFAR100]\n",
    "# )\n",
    "\n",
    "benchmark = SplitCIFAR100(\n",
    "    n_experiences=50,  # 5 incremental experiences\n",
    "    return_task_id=True,  # add task labels\n",
    "    seed=1  # you can set the seed for reproducibility. This will fix the order of classes\n",
    ")\n",
    "\n",
    "\n",
    "print(f\"{benchmark.train_stream.name} - len {len(benchmark.train_stream)}\")\n",
    "print(f\"{benchmark.test_stream.name} - len {len(benchmark.test_stream)}\")\n",
    "print(f\"{benchmark._streams}\")\n",
    "\n",
    "################################\n",
    "#    DEFINE THE OPTIMISER      #\n",
    "################################\n",
    "optimizer_KAN = SGD(model_KAN.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_incremental = SGD(model_incremental.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_multihead = SGD(model_multihead.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "################################\n",
    "#       DEFINE THE LOSS        #\n",
    "################################\n",
    "criterion_KAN = CrossEntropyLoss()\n",
    "criterion_incremental = CrossEntropyLoss()\n",
    "criterion_multihead = CrossEntropyLoss()\n",
    "\n",
    "################################\n",
    "# DEFINE THE EVALUATION PLUGIN #\n",
    "################################\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=benchmark.n_classes, save_image=False, stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False\n",
    ")\n",
    "\n",
    "################################\n",
    "#      CREATE THE STRATEGY     #\n",
    "################################\n",
    "cl_strategy_KAN = Naive(\n",
    "    model=model_KAN, optimizer=optimizer_KAN, criterion=criterion_KAN,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "cl_strategy_incremental = Naive(\n",
    "    model=model_incremental, optimizer=optimizer_incremental, criterion=criterion_incremental,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "cl_strategy_multihead = Naive(\n",
    "    model=model_multihead, optimizer=optimizer_multihead, criterion=criterion_multihead,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "################################\n",
    "#        TRAINING LOOP         #\n",
    "################################\n",
    "\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    print(f\"Experience {experience}\")\n",
    "    res = cl_strategy_KAN.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy_KAN.eval(benchmark.test_stream))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'benchmark_with_validation_stream' from 'avalanche.benchmarks.scenarios.dataset_scenario' (/home/thealch3mist/University/Thesis/venv/lib/python3.12/site-packages/avalanche/benchmarks/scenarios/dataset_scenario.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mavalanche\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbenchmarks\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mscenarios\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset_scenario\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m benchmark_with_validation_stream\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moriginal training samples = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(benchmark\u001b[38;5;241m.\u001b[39mtrain_stream[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdataset)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m benchmark \u001b[38;5;241m=\u001b[39m benchmark_with_validation_stream(benchmark, validation_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.25\u001b[39m)\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'benchmark_with_validation_stream' from 'avalanche.benchmarks.scenarios.dataset_scenario' (/home/thealch3mist/University/Thesis/venv/lib/python3.12/site-packages/avalanche/benchmarks/scenarios/dataset_scenario.py)"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_with_validation_stream\n",
    "\n",
    "\n",
    "print(f\"original training samples = {len(benchmark.train_stream[0].dataset)}\")\n",
    "\n",
    "benchmark = benchmark_with_validation_stream(benchmark, validation_size=0.25)\n",
    "print(f\"new training samples = {len(benchmark.train_stream[0].dataset)}\")\n",
    "print(f\"validation samples = {len(benchmark.valid_stream[0].dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Stream: train\n",
      "EID=0, classes=[5, 6], tasks=[0]\n",
      "data: 11339 samples\n",
      "EID=1, classes=[1, 2], tasks=[1]\n",
      "data: 12700 samples\n",
      "EID=2, classes=[0, 8], tasks=[2]\n",
      "data: 11774 samples\n",
      "EID=3, classes=[9, 3], tasks=[3]\n",
      "data: 12080 samples\n",
      "EID=4, classes=[4, 7], tasks=[4]\n",
      "data: 12107 samples\n"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "\n",
    "benchmark1 = SplitMNIST(\n",
    "    n_experiences=5,\n",
    "    return_task_id=True,  # add task labels\n",
    "    seed=1\n",
    ")\n",
    "\n",
    "print(f'--- Stream: {benchmark1.train_stream.name}')\n",
    "\n",
    "for exp in benchmark1.train_stream:\n",
    "    eid = exp.current_experience # experience id\n",
    "    clss = exp.classes_in_this_experience # classes in this experience\n",
    "    tls = exp.task_labels # task labels\n",
    "    print(f\"EID={eid}, classes={clss}, tasks={tls}\")\n",
    "    print(f\"data: {len(exp.dataset)} samples\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thealch3mist/University/Thesis/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 18\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mavalanche\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluation\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m forgetting_metrics, \\\n\u001b[1;32m     14\u001b[0m accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \\\n\u001b[1;32m     15\u001b[0m confusion_matrix_metrics, disk_usage_metrics\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mavalanche\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimpleMLP\n\u001b[0;32m---> 18\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m################################\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# DEFINE THE MODELS TO BE USED #\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m################################\u001b[39;00m\n\u001b[1;32m     23\u001b[0m model_KAN \u001b[38;5;241m=\u001b[39m FastKAN(layers_hidden\u001b[38;5;241m=\u001b[39m[\u001b[38;5;241m28\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m10\u001b[39m], num_grids\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC  # and many more!\n",
    "from avalanche.training.templates import SupervisedTemplate\n",
    "from avalanche.training.plugins import EvaluationPlugin, ReplayPlugin, EWCPlugin\n",
    "\n",
    "from avalanche.logging import InteractiveLogger\n",
    "\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.optim import SGD\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "from avalanche.evaluation.metrics import forgetting_metrics, \\\n",
    "accuracy_metrics, loss_metrics, timing_metrics, cpu_usage_metrics, \\\n",
    "confusion_matrix_metrics, disk_usage_metrics\n",
    "from avalanche.models import SimpleMLP\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "################################\n",
    "# DEFINE THE MODELS TO BE USED #\n",
    "################################\n",
    "model_KAN = FastKAN(layers_hidden=[28*28,32,10], num_grids=4, device=device)\n",
    "model_incremental = IncrementalClassifier(in_features=28*28)\n",
    "model_multihead = MultiHeadClassifier(in_features=28*28)\n",
    "\n",
    "################################\n",
    "#    DEFINE THE OPTIMISER      #\n",
    "################################\n",
    "optimizer_KAN = SGD(model_KAN.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_incremental = SGD(model_incremental.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_multihead = SGD(model_multihead.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "################################\n",
    "#       DEFINE THE LOSS        #\n",
    "################################\n",
    "criterion_KAN = CrossEntropyLoss()\n",
    "criterion_incremental = CrossEntropyLoss()\n",
    "criterion_multihead = CrossEntropyLoss()\n",
    "\n",
    "################################\n",
    "# DEFINE THE EVALUATION PLUGIN #\n",
    "################################\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    accuracy_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True),\n",
    "    forgetting_metrics(experience=True, stream=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    confusion_matrix_metrics(num_classes=benchmark.n_classes, save_image=False, stream=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[InteractiveLogger()],\n",
    "    strict_checks=False\n",
    ")\n",
    "\n",
    "################################\n",
    "#      CREATE THE STRATEGY     #\n",
    "################################\n",
    "cl_strategy_KAN = Naive(\n",
    "    model_KAN, optimizer_KAN, criterion_KAN,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "cl_strategy_incremental = Naive(\n",
    "    model_incremental, optimizer_incremental, criterion_incremental,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "cl_strategy_multihead = Naive(\n",
    "    model_multihead, optimizer_multihead, criterion_multihead,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100, evaluator=eval_plugin\n",
    ")\n",
    "\n",
    "################################\n",
    "#        TRAINING LOOP         #\n",
    "################################\n",
    "\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for experience in benchmark.train_stream:\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy_KAN.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy_KAN.eval(benchmark.test_stream))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting experiment...\n",
      "Start of experience:  0\n",
      "Current Classes:  [5, 6]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cl_strategy_KAN' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart of experience: \u001b[39m\u001b[38;5;124m\"\u001b[39m, experience\u001b[38;5;241m.\u001b[39mcurrent_experience)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurrent Classes: \u001b[39m\u001b[38;5;124m\"\u001b[39m, experience\u001b[38;5;241m.\u001b[39mclasses_in_this_experience)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mcl_strategy_KAN\u001b[49m\u001b[38;5;241m.\u001b[39mtrain(experience)\n\u001b[1;32m     17\u001b[0m cl_strategy_incremental\u001b[38;5;241m.\u001b[39mtrain(experience)\n\u001b[1;32m     18\u001b[0m cl_strategy_multihead\u001b[38;5;241m.\u001b[39mtrain(experience)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cl_strategy_KAN' is not defined"
     ]
    }
   ],
   "source": [
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "\n",
    "# scenario\n",
    "benchmark = SplitMNIST(n_experiences=5, seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results_KAN = []\n",
    "results_incremental = []\n",
    "results_multihead = []\n",
    "\n",
    "for experience in benchmark.train_stream:\n",
    "    print(\"Start of experience: \", experience.current_experience)\n",
    "    print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy_KAN.train(experience)\n",
    "    cl_strategy_incremental.train(experience)\n",
    "    cl_strategy_multihead.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    results_KAN.append(cl_strategy_KAN.eval(benchmark.test_stream))\n",
    "    results_incremental.append(cl_strategy_incremental.eval(benchmark.test_stream))\n",
    "    results_multihead.append(cl_strategy_multihead.eval(benchmark.test_stream))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/thealch3mist/University/Thesis/venv/lib/python3.12/site-packages/avalanche/training/templates/base.py:468: PositionalArgumentsDeprecatedWarning: Avalanche is transitioning to strategy constructors that accept named (keyword) arguments only. This is done to ensure that there is no confusion regarding the meaning of each argument (strategies can have many arguments). Your are passing 3 positional arguments to the Naive.__init__ method. Consider passing them as names arguments. The ability to pass positional arguments will be removed in the future.\n",
      "  warnings.warn(error_str, category=PositionalArgumentsDeprecatedWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- >> Start of training phase << --\n",
      "100%|██████████| 114/114 [00:01<00:00, 61.15it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.4083\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8932\n",
      "100%|██████████| 114/114 [00:01<00:00, 59.17it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1044\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9692\n",
      "100%|██████████| 114/114 [00:01<00:00, 59.15it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0837\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9742\n",
      "100%|██████████| 114/114 [00:01<00:00, 57.87it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0753\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9763\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 69.28it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0652\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.9789\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 70.09it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 7.7935\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 65.14it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 10.6690\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 64.63it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 10.0777\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 61.66it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 8.4739\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 7.5236\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1811\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 127/127 [00:02<00:00, 62.66it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.6304\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8694\n",
      "100%|██████████| 127/127 [00:02<00:00, 61.95it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0667\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9811\n",
      "100%|██████████| 127/127 [00:02<00:00, 58.75it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0527\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9843\n",
      "100%|██████████| 127/127 [00:02<00:00, 62.79it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0459\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9866\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 72.24it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.2041\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 72.28it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0313\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.9922\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 72.25it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 10.3574\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0000\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 73.72it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 8.9574\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 67.61it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 8.3247\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 6.8452\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2150\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 118/118 [00:01<00:00, 61.67it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.7943\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8483\n",
      "100%|██████████| 118/118 [00:01<00:00, 60.30it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0631\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9818\n",
      "100%|██████████| 118/118 [00:02<00:00, 58.61it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0476\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9858\n",
      "100%|██████████| 118/118 [00:01<00:00, 62.80it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0410\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9883\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 71.99it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 6.4701\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 66.68it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.8074\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 72.83it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0283\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.9928\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 76.44it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 8.6693\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0000\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 75.79it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 7.5130\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.7214\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1940\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 121/121 [00:01<00:00, 61.17it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.8585\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8180\n",
      "100%|██████████| 121/121 [00:02<00:00, 56.54it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.1172\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9626\n",
      "100%|██████████| 121/121 [00:02<00:00, 53.24it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0964\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9708\n",
      "100%|██████████| 121/121 [00:02<00:00, 57.50it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0832\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9745\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 68.56it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.4134\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 62.54it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 6.4463\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 70.04it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.9960\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0005\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 72.31it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0618\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.9817\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 69.36it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 9.1429\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0000\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.9856\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.1983\n",
      "-- >> Start of training phase << --\n",
      "100%|██████████| 122/122 [00:02<00:00, 59.21it/s]\n",
      "Epoch 0 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.9312\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.8125\n",
      "100%|██████████| 122/122 [00:02<00:00, 54.82it/s]\n",
      "Epoch 1 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0973\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9733\n",
      "100%|██████████| 122/122 [00:02<00:00, 46.84it/s]\n",
      "Epoch 2 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0713\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9791\n",
      "100%|██████████| 122/122 [00:02<00:00, 49.74it/s]\n",
      "Epoch 3 ended.\n",
      "\tLoss_Epoch/train_phase/train_stream/Task000 = 0.0608\n",
      "\tTop1_Acc_Epoch/train_phase/train_stream/Task000 = 0.9822\n",
      "-- >> End of training phase << --\n",
      "-- >> Start of eval phase << --\n",
      "-- Starting eval on experience 0 (Task 0) from test stream --\n",
      "100%|██████████| 19/19 [00:00<00:00, 67.30it/s]\n",
      "> Eval on experience 0 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp000 = 7.2493\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp000 = 0.0000\n",
      "-- Starting eval on experience 1 (Task 0) from test stream --\n",
      "100%|██████████| 22/22 [00:00<00:00, 69.84it/s]\n",
      "> Eval on experience 1 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp001 = 5.4521\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp001 = 0.0000\n",
      "-- Starting eval on experience 2 (Task 0) from test stream --\n",
      "100%|██████████| 20/20 [00:00<00:00, 68.66it/s]\n",
      "> Eval on experience 2 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp002 = 6.6581\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp002 = 0.0010\n",
      "-- Starting eval on experience 3 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 65.67it/s]\n",
      "> Eval on experience 3 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp003 = 6.5889\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp003 = 0.0411\n",
      "-- Starting eval on experience 4 (Task 0) from test stream --\n",
      "100%|██████████| 21/21 [00:00<00:00, 67.88it/s]\n",
      "> Eval on experience 4 (Task 0) from test stream ended.\n",
      "\tLoss_Exp/eval_phase/test_stream/Task000/Exp004 = 0.0400\n",
      "\tTop1_Acc_Exp/eval_phase/test_stream/Task000/Exp004 = 0.9876\n",
      "-- >> End of eval phase << --\n",
      "\tLoss_Stream/eval_phase/test_stream/Task000 = 5.1619\n",
      "\tTop1_Acc_Stream/eval_phase/test_stream/Task000 = 0.2070\n"
     ]
    }
   ],
   "source": [
    "from torch.optim import SGD\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.models import SimpleMLP\n",
    "from avalanche.training.supervised import Naive, CWRStar, Replay, GDumb, Cumulative, LwF, GEM, AGEM, EWC  # and many more!\n",
    "\n",
    "model = SimpleMLP(num_classes=10)\n",
    "optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = CrossEntropyLoss()\n",
    "cl_strategy = Naive(\n",
    "    model, optimizer, criterion,\n",
    "    train_mb_size=100, train_epochs=4, eval_mb_size=100\n",
    ")\n",
    "from avalanche.benchmarks.classic import SplitMNIST\n",
    "\n",
    "# scenario\n",
    "benchmark = SplitMNIST(n_experiences=5, seed=1)\n",
    "\n",
    "# TRAINING LOOP\n",
    "# print('Starting experiment...')\n",
    "results = [] \n",
    "for experience in benchmark.train_stream:\n",
    "    # print(\"Start of experience: \", experience.current_experience)\n",
    "    # print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    cl_strategy.train(experience)\n",
    "    # print('Training completed')\n",
    "\n",
    "    # print('Computing accuracy on the whole test set')\n",
    "    results.append(cl_strategy.eval(benchmark.test_stream))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9744245524296675, 'Loss_Epoch/train_phase/train_stream/Task000': 0.07835279670509883, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.9816216216216216, 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 0.06574291653452895, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 7.920903306212027, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 10.382590512536321, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 10.082500989629585, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 8.420142700423055, 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.1816, 'Loss_Stream/eval_phase/test_stream/Task000': 7.485486004755879}\n",
      "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9878740157480315, 'Loss_Epoch/train_phase/train_stream/Task000': 0.043713358560885034, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.165399396741712, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.9921550530687586, 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 0.031237828650137467, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 10.016829230036194, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 8.669862181317281, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 8.02095937254417, 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.215, 'Loss_Stream/eval_phase/test_stream/Task000': 6.652314565704111}\n",
      "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9877696619670461, 'Loss_Epoch/train_phase/train_stream/Task000': 0.04119873469015506, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 6.570904267800821, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 5.997596198092093, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.9912998976458547, 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 0.029418819103039184, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 9.33338280158683, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 7.66247581368062, 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.1937, 'Loss_Stream/eval_phase/test_stream/Task000': 5.945612449112628}\n",
      "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9755794701986755, 'Loss_Epoch/train_phase/train_stream/Task000': 0.08193237809094274, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.2700068886215625, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 6.4038684204200145, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 6.988290725247277, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.9767211490837048, 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 0.06567646625971606, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 9.295024871826172, 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.1972, 'Loss_Stream/eval_phase/test_stream/Task000': 5.979741646588221}\n",
      "{'Top1_Acc_Epoch/train_phase/train_stream/Task000': 0.9837284215742959, 'Loss_Epoch/train_phase/train_stream/Task000': 0.05713071651485067, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp000': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp000': 7.648380008903709, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp001': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp001': 5.647789751377642, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp002': 0.0, 'Loss_Exp/eval_phase/test_stream/Task000/Exp002': 6.776435952103614, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp003': 0.0633977216443784, 'Loss_Exp/eval_phase/test_stream/Task000/Exp003': 6.7662242542935696, 'Top1_Acc_Exp/eval_phase/test_stream/Task000/Exp004': 0.9890547263681592, 'Loss_Exp/eval_phase/test_stream/Task000/Exp004': 0.03897207343269751, 'Top1_Acc_Stream/eval_phase/test_stream/Task000': 0.2116, 'Loss_Stream/eval_phase/test_stream/Task000': 5.3368759895136115}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in results:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
